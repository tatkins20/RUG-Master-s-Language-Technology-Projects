{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction to Feedforward Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "After this lecture you should:\n",
    "* know what a neural network is\n",
    "* understand its basic building blocks\n",
    "* understand why we need non-linearities\n",
    "* connect different views on neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A neural network is a computational model that has slightly different meanings in different communitites."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* **Cognitive science view**: a computational model of the brain consisting of artificial neural perceptrons\n",
    "* **Machine Learning view**\n",
    "   * **Linear algebra view**: a network of perceptron-like nodes, i.e., a set of matrix multiplication operations\n",
    "   * **Graph theory view**: a computational graph model (with automatic differentiation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "As the name already suggests, a neural network is a network. It can be seen as a model that is build up from basic building blocks. Lets first look at one such building block, for instance, a single perceptron. \n",
    "\n",
    "<img src=\"pics/lego.jpg\" width=300>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## From biological neurons to artificial neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "To get started, we will introduce the **perceptron**. It was introduced with the well-known perceptron algorithm by Rosenblatt (1957), inspired by earlier work on McCulloch-Pitts to model neurons in the brain. In layman's terms, a neuron gets information through dendrites and if enough information is accumulated the neuron 'fires' and sends information down the axon: \n",
    "\n",
    "<div style=\"width: 25%;\">\n",
    "<img src=\"pics/neuron.jpg\">\n",
    "<img src=\"pics/neuron-simple.png\">\n",
    "</div>\n",
    "\n",
    "Thus neural networks are biologically inspired."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How does the perceptron work?\n",
    "The basic perceptron gets $n$ **inputs** $x_1,..,x_n$ and produces an **output** $y$. It does so by **weighting** the inputs by $w_1,..,w_n$, sums up the weighted inputs and sends this weighted sum through an **activation function** $\\sigma$ to see if the neuron \"fires\". That is, if the weighted sum is above a **threshold** the output will be 1, otherwise 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Mathematically, the perceptron is formulated as: \n",
    "\n",
    "$y = \\sigma(\\sum_{j=1}^n w_{j} x_j )$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We can visualize the perceptron as (for a given perceptron node $k$):\n",
    "<div style=\"width: 35%;\"><img src=\"pics/perceptron.png\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is $\\sigma$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In the perceptron $\\sigma$ is a **threshold** function. Intuitively, the perceptron only fires if the weighted sum is above a given threshold. We can formulate this intuition as:\n",
    "\n",
    "\n",
    "$$\\begin{equation}\n",
    "    y=\n",
    "    \\begin{cases}\n",
    "      1 & \\text{if} (\\sum_j w_j x_j) > threshold\\\\\n",
    "      0 & \\text{otherwise}\\\\\n",
    "    \\end{cases}\n",
    "  \\end{equation}$$\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's rewrite the equation of the perceptron. First, notice that $\\sum_{j=1} w_{j} x_j $ is the **dot product** of the weights and input, and can be written as: \n",
    "\n",
    "$$\\sum_{j=1} w_{j} x_j = \\vec{w} \\cdot \\vec{x}$$ where $\\vec{w}$ and $\\vec{x}$ are now vectors. If it is clear from the context then we avoid the explicit vector notation and simply write $w \\cdot x$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Second, we will move the threshold inside the equation by introducing a bias term $b=-threshold$. Using these two changes, the equation rewrites as:\n",
    "\n",
    "$$\\begin{equation}\n",
    "    y=\n",
    "    \\begin{cases}\n",
    "      1 & \\text{if} (w \\cdot x + b) > 0)\\\\\n",
    "      0 & \\text{otherwise}\\\\\n",
    "    \\end{cases}\n",
    "  \\end{equation}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example\n",
    "Suppose we have a perceptron with two inputs, weights -2 and -2 and bias term 3. <img src=\"pics/nand-graph.png\">\n",
    "What function does this perceptron compute? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "def compute(x1, x2):\n",
    "    a = x1*-2 + x2*-2 + 1 * 3\n",
    "    if a > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "x1=0\n",
    "x2=0\n",
    "print(compute(x1,x2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Vectorization\n",
    "Instead of this cumbersome notation, let's use vectorization. Now we represent our input instances as vectors, and the entire data as a matrix. Also the weights are a vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3  1  1 -1]\n",
      "[1, 1, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "inputs = np.array([[0,0],\n",
    "                   [0,1],\n",
    "                   [1,0],\n",
    "                   [1,1]])\n",
    "def compute(input_matrix):\n",
    "    W = [-2,-2]\n",
    "    b = 3\n",
    "    a = np.dot(input_matrix,W) + b\n",
    "    print(a) # debug\n",
    "    return [1 if elem > 0 else 0 for elem in a]\n",
    "labels=compute(inputs)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Beautiful! Now we have a perceptron that models the NAND logical function. That is, it returns 0 only if both inputs are active (not AND)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Visualization\n",
    "\n",
    "We can visualize the example by looking at where the input vectors are in the space and which label they get. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"pics/nand-plot.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Exercise (try at home):** try to manually modify the parameters (the weights and bias) of the little perceptron network. Can you get another logical function? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAD8CAYAAACLrvgBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAFohJREFUeJzt3X+wXGd93/H3BwmbMfmBjAQ4tmXLrcKvDNiwKAlkwE4wCNqx6IQBuaYRwYwGJ04nZdKJqRLomHpqyB8wmVDgxhUYqtoYt4DShBJji+EPEOgqNf7V2BIy2HfkYIH5MR4xdmW+/WOPwur6Xt3navfuXoX3a2Znz3me55zzvc+u7ueePburVBWSJLV4yqQLkCSdPAwNSVIzQ0OS1MzQkCQ1MzQkSc0MDUlSs5GERpLtSR5Octc8/Rcm+WGS27vbuwf6Nia5N8n+JFeNoh5J0tLIKD6nkeSVwKPAJ6rqV+bovxD4o6r6l7PaVwD3ARcDM8Ae4NKqumfooiRJIzeSM42q+jLwyAlsugHYX1UHqupx4EZg0yhqkiSN3soxHuvXk3wDOEj/rONu4EzgwYExM8CvzrVxkq3AVoCnP/3pL33e8563xOVK0j8te/fu/W5VrRlmH+MKjb8DzqmqR5O8HvgssB7IHGPnfL2sqqaAKYBer1fT09NLVask/ZOU5NvD7mMs756qqh9V1aPd8t8AT02ymv6ZxdkDQ8+ifyYiSVqGxhIaSZ6TJN3yhu6436N/4Xt9knVJTgE2AzvHUZMkafFG8vJUkhuAC4HVSWaA9wBPBaiqjwBvBK5IcgT4MbC5+m/bOpLkSuALwApge3etQ5K0DI3kLbfj5jUNSVq8JHurqjfMPvxEuCSpmaEhSWpmaEiSmhkakqRmhoYkqZmhIUlqZmhIkpoZGpKkZoaGJKmZoSFJamZoSJKaGRqSpGaGhiSpmaEhSWpmaEiSmhkakqRmhoYkqZmhIUlqNpLQSLI9ycNJ7pqn/7Ikd3S3ryR58UDft5LcmeT2JP4frpK0jI3qTOPjwMbj9N8PvKqqXgS8F5ia1X9RVZ0/7P9dK0laWitHsZOq+nKSc4/T/5WB1d3AWaM4riRpvCZxTeNy4PMD6wX8bZK9SbZOoB5JUqORnGm0SnIR/dD4jYHmV1TVwSTPAm5J8vdV9eU5tt0KbAVYu3btWOqVJB1rbGcaSV4EXAdsqqrvHW2vqoPd/cPAZ4ANc21fVVNV1auq3po1a8ZRsiRplrGERpK1wP8E/k1V3TfQ/vQkP390GXgNMOc7sCRJkzeSl6eS3ABcCKxOMgO8B3gqQFV9BHg38EzgvyQBONK9U+rZwGe6tpXAf6+q/z2KmiRJozeqd09dukD/24G3z9F+AHjxk7eQJC1HfiJcktTM0JAkNTM0JEnNDA1JUjNDQ5LUzNCQJDUzNCRJzQwNSVIzQ0OS1MzQkCQ1MzQkSc0MDUlSM0NDktTM0JAkNTM0JEnNDA1JUjNDQ5LUzNCQJDUzNCRJzUYSGkm2J3k4yV3z9CfJnyfZn+SOJC8Z6NuSZF932zKKetRmxw4491x4ylP69zt2TLoiCXj/+2HXrmPbdu3qt2viRnWm8XFg43H6Xwes725bgQ8DJDkdeA/wq8AG4D1JVo2oJh3Hjh2wdSt8+9tQ1b/futXg0DLwspfBm9700+DYtau//rKXTbYuASMKjar6MvDIcYZsAj5RfbuBZyQ5A3gtcEtVPVJV3wdu4fjhoxHZtg0OHz627fDhfrs0URddBDfd1A+Kd7+7f3/TTf12Tdy4rmmcCTw4sD7Ttc3X/iRJtiaZTjJ96NChJSv0Z8UDDyyuXRqriy6CK66A9763f29gLBvjCo3M0VbHaX9yY9VUVfWqqrdmzZqRFvezaO3axbVLY7VrF3z4w/Cnf9q/n32NQxMzrtCYAc4eWD8LOHicdi2xa66B0047tu200/rt0kQdvYZx001w9dU/fanK4FgWxhUaO4Hf6d5F9WvAD6vqIeALwGuSrOougL+ma9MSu+wymJqCc86BpH8/NdVvlyZqz55jr2EcvcaxZ89k6xIAqZrz1aDF7SS5AbgQWA18h/47op4KUFUfSRLgL+hf5D4M/G5VTXfbvg34D92urqmqjy10vF6vV9PT00PXLUk/S5LsrareMPtYOYpCqurSBfoL+P15+rYD20dRhyRpafmJcElSM0NDktTM0JAkNTM0JEnNDA1JUjNDQ5LUzNCQJDUzNCRJzQwNSVIzQ0OS1MzQkCQ1MzQkSc0MDUlSM0NDktTM0JAkNTM0JEnNDA1JUjNDQ5LUbCShkWRjknuT7E9y1Rz9H0hye3e7L8kPBvqeGOjbOYp6JElLY+j/IzzJCuBDwMXADLAnyc6quufomKr6dwPj/wC4YGAXP66q84etQ5K09EZxprEB2F9VB6rqceBGYNNxxl8K3DCC40qSxmwUoXEm8ODA+kzX9iRJzgHWAbcNND8tyXSS3UneMN9Bkmztxk0fOnRoBGVLkhZrFKGROdpqnrGbgZur6omBtrVV1QP+NfDBJP9srg2raqqqelXVW7NmzXAVS5JOyChCYwY4e2D9LODgPGM3M+ulqao62N0fAL7Esdc7JEnLyChCYw+wPsm6JKfQD4YnvQsqyXOBVcBXB9pWJTm1W14NvAK4Z/a2kqTlYeh3T1XVkSRXAl8AVgDbq+ruJFcD01V1NEAuBW6sqsGXrp4PfDTJT+gH2LWD77qSJC0vOfZ3+Mmh1+vV9PT0pMuQpJNKkr3dNeQT5ifCJUnNDA1JUjNDQ5LUzNCQJDUzNCRJzQwNSVIzQ0OS1MzQkCQ1MzQkSc0MDUlSM0NDktTM0JAkNTM0JEnNDA1JUjNDQ5LUzNCQJDUzNCRJzQwNSVKzkYRGko1J7k2yP8lVc/S/NcmhJLd3t7cP9G1Jsq+7bRlFPZKkpbFy2B0kWQF8CLgYmAH2JNlZVffMGvqpqrpy1ranA+8BekABe7ttvz9sXZKk0RvFmcYGYH9VHaiqx4EbgU2N274WuKWqHumC4hZg4whqkiQtgVGExpnAgwPrM13bbL+d5I4kNyc5e5HbkmRrkukk04cOHRpB2ZKkxRpFaGSOtpq1/lfAuVX1IuCLwPWL2LbfWDVVVb2q6q1Zs+aEi5UknbhRhMYMcPbA+lnAwcEBVfW9qnqsW/1L4KWt20qSlo9RhMYeYH2SdUlOATYDOwcHJDljYPUS4P92y18AXpNkVZJVwGu6NknSMjT0u6eq6kiSK+n/sl8BbK+qu5NcDUxX1U7g3ya5BDgCPAK8tdv2kSTvpR88AFdX1SPD1iRJWhqpmvMSwrLW6/Vqenp60mVI0kklyd6q6g2zDz8RLklqZmhIkpoZGpKkZoaGJKmZoSFJamZoSJKaGRqSpGaGhiSpmaEhSWpmaEiSmhkakqRmhoYkqZmhIUlqZmhIkpoZGpKkZoaGJKmZoSFJamZoSJKajSQ0kmxMcm+S/UmumqP/nUnuSXJHkluTnDPQ90SS27vbzlHUI0laGiuH3UGSFcCHgIuBGWBPkp1Vdc/AsP8D9KrqcJIrgPcDb+76flxV5w9bhyRp6Y3iTGMDsL+qDlTV48CNwKbBAVW1q6oOd6u7gbNGcFxJ0piNIjTOBB4cWJ/p2uZzOfD5gfWnJZlOsjvJG+bbKMnWbtz0oUOHhqtYknRChn55CsgcbTXnwOQtQA941UDz2qo6mOQ84LYkd1bVN5+0w6opYAqg1+vNuX9J0tIaxZnGDHD2wPpZwMHZg5K8GtgGXFJVjx1tr6qD3f0B4EvABSOoSZK0BEYRGnuA9UnWJTkF2Awc8y6oJBcAH6UfGA8PtK9Kcmq3vBp4BTB4AV2StIwM/fJUVR1JciXwBWAFsL2q7k5yNTBdVTuBPwN+Dvh0EoAHquoS4PnAR5P8hH6AXTvrXVeSpGUkVSff5YFer1fT09OTLkOSTipJ9lZVb5h9+IlwSVIzQ0OS1MzQkCQ1MzQkSc0MDUlSM0NDktTM0JAkNTM0JEnNDA1JUjNDQ5LUzNCQJDUzNCRJzQwNSVIzQ0OS1MzQkCQ1MzQkSc0MDUlSM0NDktRsJKGRZGOSe5PsT3LVHP2nJvlU1/+1JOcO9L2ra783yWtHUY/a7NgB554LT3lK/37HjklXJPX53Fy+Vg67gyQrgA8BFwMzwJ4kO6vqnoFhlwPfr6p/nmQz8D7gzUleAGwGXgj8EvDFJL9cVU8MW5eOb8cO2LoVDh/ur3/72/11gMsum1xdks/N5W0UZxobgP1VdaCqHgduBDbNGrMJuL5bvhn4rSTp2m+sqseq6n5gf7c/LbFt2376j/Kow4f77dIk+dxc3kYRGmcCDw6sz3Rtc46pqiPAD4FnNm4LQJKtSaaTTB86dGgEZf9se+CBxbVL4+Jzc3kbRWhkjrZqHNOybb+xaqqqelXVW7NmzSJL1Gxr1y6uXRoXn5vL2yhCYwY4e2D9LODgfGOSrAR+EXikcVstgWuugdNOO7bttNP67dIk+dxc3kYRGnuA9UnWJTmF/oXtnbPG7AS2dMtvBG6rquraN3fvrloHrAe+PoKatIDLLoOpKTjnHEj691NTXmjU5PncXN7S/9095E6S1wMfBFYA26vqmiRXA9NVtTPJ04BPAhfQP8PYXFUHum23AW8DjgB/WFWfX+h4vV6vpqenh65bkn6WJNlbVb2h9jGK0Bg3Q0OSFm8UoeEnwiVJzQwNSVIzQ0OS1MzQkCQ1MzQkSc0MDUlSM0NDktTM0JAkNTM0JEnNDA1JUjNDQ5LUzNCQJDUzNCRJzQwNSVIzQ0OS1MzQkCQ1MzQkSc0MDUlSs6FCI8npSW5Jsq+7XzXHmPOTfDXJ3UnuSPLmgb6PJ7k/ye3d7fxh6pEkLa1hzzSuAm6tqvXArd36bIeB36mqFwIbgQ8mecZA/7+vqvO72+1D1iNJWkLDhsYm4Ppu+XrgDbMHVNV9VbWvWz4IPAysGfK4kqQJGDY0nl1VDwF098863uAkG4BTgG8ONF/TvWz1gSSnDlmPJGkJrVxoQJIvAs+Zo2vbYg6U5Azgk8CWqvpJ1/wu4B/oB8kU8MfA1fNsvxXYCrB27drFHFqSNCILhkZVvXq+viTfSXJGVT3UhcLD84z7BeCvgT+pqt0D+36oW3wsyceAPzpOHVP0g4Ver1cL1S1JGr1hX57aCWzplrcAn5s9IMkpwGeAT1TVp2f1ndHdh/71kLuGrEeStISGDY1rgYuT7AMu7tZJ0ktyXTfmTcArgbfO8dbaHUnuBO4EVgP/ach6JElLKFUn3ys9vV6vpqenJ12GJJ1Ukuytqt4w+/AT4ZKkZoaGJKmZoSFJamZoSJKaGRqSpGaGhiSpmaEhSWpmaEiSmhkakqRmhoYkqZmhIUlqZmhIkpoZGpKkZoaGJKmZoSFJamZoSJKaGRqSpGaGhiSpmaEhSWo2VGgkOT3JLUn2dfer5hn3RJLbu9vOgfZ1Sb7Wbf+pJKcMU48kaWkNe6ZxFXBrVa0Hbu3W5/Ljqjq/u10y0P4+4APd9t8HLh+yHknSEho2NDYB13fL1wNvaN0wSYDfBG4+ke0lSeO3csjtn11VDwFU1UNJnjXPuKclmQaOANdW1WeBZwI/qKoj3ZgZ4Mz5DpRkK7C1W30syV1D1j4Oq4HvTrqIBidDnSdDjWCdo2ado/XcYXewYGgk+SLwnDm6ti3iOGur6mCS84DbktwJ/GiOcTXfDqpqCpjqapquqt4ijj8R1jk6J0ONYJ2jZp2j1f3xPpQFQ6OqXn2cAr6T5IzuLOMM4OF59nGwuz+Q5EvABcD/AJ6RZGV3tnEWcPAEfgZJ0pgMe01jJ7ClW94CfG72gCSrkpzaLa8GXgHcU1UF7ALeeLztJUnLx7ChcS1wcZJ9wMXdOkl6Sa7rxjwfmE7yDfohcW1V3dP1/THwziT76V/j+K+Nx50asu5xsc7RORlqBOscNescraHrTP8PfkmSFuYnwiVJzQwNSVKzZRsaJ8NXlLTUmOT8JF9NcneSO5K8eaDv40nuH6j//BHXtzHJvUn2J3nSp/WTnNrNzf5urs4d6HtX135vkteOsq4TqPOdSe7p5u/WJOcM9M35+E+ozrcmOTRQz9sH+rZ0z5N9SbbM3nbMdX5goMb7kvxgoG8s85lke5KH5/u8Vfr+vPsZ7kjykoG+cc7lQnVe1tV3R5KvJHnxQN+3ktzZzeXQb3Udss4Lk/xw4LF990DfcZ8vT1JVy/IGvB+4qlu+CnjfPOMenaf9JmBzt/wR4IpJ1Aj8MrC+W/4l4CHgGd36x4E3LtH8rQC+CZwHnAJ8A3jBrDG/B3ykW94MfKpbfkE3/lRgXbefFROs8yLgtG75iqN1Hu/xn1CdbwX+Yo5tTwcOdPeruuVVk6pz1vg/ALZPYD5fCbwEuGue/tcDnwcC/BrwtXHPZWOdLz96fOB1R+vs1r8FrF4m83kh8L+Gfb5U1fI90+Dk+IqSBWusqvuqal+3fJD+Z1nWLEEts20A9lfVgap6HLixq3fQYP03A7/Vzd0m4Maqeqyq7gf2d/ubSJ1VtauqDneru+l/pmfcWuZzPq8FbqmqR6rq+8AtwMZlUuelwA1LVMu8qurLwCPHGbIJ+ET17ab/ma4zGO9cLlhnVX2lqwMm99xsmc/5LPp5vZxD45ivKAGO+xUlSXYnOfpLe1FfUTKGGgFIsoF+mn9zoPma7tT2A+k+zzIiZwIPDqzPNQf/OKabqx/Sn7uWbcdZ56DL6f8FetRcj/9SaK3zt7vH8+YkZy9y21FoPlb3Mt864LaB5nHN50Lm+znGOZeLNfu5WcDfJtmb/tcgTdqvJ/lGks8neWHXtuj5HPa7p4aSZfIVJWOoke6vpE8CW6rqJ13zu4B/oB8kU/Q/t3L1idQ51yHnaJs9B/ONadl2VJqPleQtQA941UDzkx7/qvrmXNuPoc6/Am6oqseSvIP+WdxvNm47Kos51mbg5qp6YqBtXPO5kOXw3GyW5CL6ofEbA82v6ObyWcAtSf6+OyOYhL8DzqmqR5O8HvgssJ4TmM+JnmlU1aur6lfmuH0O+E73i/boL9wFv6IE+BL9ryj5Lt1XlHTDTvgrSkZRY5JfAP4a+JPuVPvovh/qTr8fAz7GaF8CmgHOHlifaw7+cUw3V79I/xS3Zdtx1kmSV9MP6ku6+QLmffwnUmdVfW+gtr8EXtq67TjrHLCZWS9NjXE+FzLfzzHOuWyS5EXAdcCmqvre0faBuXwY+AxL9xLvgqrqR1X1aLf8N8BT0/+GjsXP5zgu0pzIDfgzjr3I/P45xqwCTu2WVwP76C7iAJ/m2AvhvzehGk+h/3+N/OEcfWd09wE+SP/T8qOqbSX9i4Tr+OkFrhfOGvP7HHsh/KZu+YUceyH8AEt3Ibylzgvov6S3vvXxn1CdZwws/ytgd7d8OnB/V++qbvn0SdXZjXsu/Qu1mcR8dsc4l/kv3P4Ljr0Q/vVxz2VjnWvpX/N7+az2pwM/P7D8FWDjBOt8ztHHmn54PdDNbdPz5Zh9LeUPMeQEPJP+L9t93f3pXXsPuK5bfjlwZ/eD3glcPrD9ecDXuwf000f/MUygxrcA/w+4feB2ftd3W1f3XcB/A35uxPW9HriP/i/cbV3b1fT/Wgd4Wjc3+7u5Om9g223ddvcCr1vix3qhOr8IfGdg/nYu9PhPqM7/DNzd1bMLeN7Atm/r5nk/8LuTrLNb/4/M+iNlnPNJ/wznoe7fxgz9l3beAbyj6w/woe5nuBPoTWguF6rzOvr/gdzR5+Z0135eN4/f6J4T2yZc55UDz83dDITcXM+X4938GhFJUrPl/O4pSdIyY2hIkpoZGpKkZoaGJKmZoSFJamZoSJKaGRqSpGb/H/nsQjtU6ff6AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "colors=['r','b']\n",
    "types=['x','o']\n",
    "for i, lab in enumerate(labels):\n",
    "    plt.plot(inputs[i,0], inputs[i,1], types[lab], color=colors[lab])\n",
    "plt.axis([-0.5, 1.5, -0.5, 1.5])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Right, it's the equation of a line! To be precise, since the inputs have usually more than 2 dimensions it is actually a **hyperplane**. Try to imagine the line in our NAND example.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Linear separability\n",
    "\n",
    "The perceptron is a **linear** classifier. Now this should be clear from the formula."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\begin{equation}\n",
    "    y=\n",
    "    \\begin{cases}\n",
    "      1 & \\text{if} (w \\cdot x + b) > 0)\\\\\n",
    "      0 & \\text{otherwise}\\\\\n",
    "    \\end{cases}\n",
    "  \\end{equation}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "what does it resemble?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Linearly separable?\n",
    "Let's have a look at the following two examples. Are they linearly separable? Which logical functions do they represent?\n",
    "<img src=\"pics/linearq.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "left: OR, right: XOR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The one on the right is not linearly separable. If the data is not linearly separable, the perceptron has a hard time. What can we do about it? There are tricks to make the perceptron work in such cases, but usually you will move to a model with higher **representational capacity**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"pics/separability.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A Neural Network for XOR\n",
    "\n",
    "The following network implements XOR, it has two layers of ReLU units (Goodfellow et al., 2016). $ReLU(x)=max(0,x)$\n",
    "\n",
    "<img src=\"pics/xor_nn.png\" width=\"30%\">\n",
    "\n",
    "* Input $x=[0,0]$\n",
    "* $H=[0,0]$\n",
    " * $h_1 = relu(0*1+0*1+0) = max(0,0) = 0$ \n",
    " * $h_2 = relu(0*1+0*1-1) = max(0,-1) = 0$\n",
    "* $y=0$\n",
    " * $y=0*1+0*(-2)+0=0$\n",
    " \n",
    "Exercise: do the calculations for the remaining inputs: $[0,1]$, $[1,0]$, $[1,1]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A Neural Network for XOR (2)\n",
    "\n",
    "The hidden representations of the two input points $x=[0,1]$ and $x=[1,0]$ are merged to the same point: $h=[1,0]$\n",
    "\n",
    "<img src=\"pics/xor_spaces.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Why Neural Networks\n",
    "\n",
    "* **non-linearity**\n",
    "* **representational power** (can represent any function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is a Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A neural network is a network of nodes. It has **input** nodes, **output node(s)** and usually **hidden nodes**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Below is a visualization of a neural network. The green nodes are the inputs, the blue nodes are **hidden nodes** and the last *layer* is the **output** layer. How many input, hidden and output nodes does this network have?\n",
    "\n",
    "A feedforward neural network:\n",
    "<img src=\"pics/nn.png\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Or another visualization (this one with 2 hidden layers): <img src=\"http://neuralnetworksanddeeplearning.com/images/tikz11.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Such a basic neural network is also called:\n",
    "* **feedforward neural network**\n",
    "* **multi-layer perceptron** (MLP) (for some odd historical reasons)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Lets look at a more detailed example. The network  can be formulated as:\n",
    "<img src=\"pics/nn.png\" width=300> \n",
    "\n",
    "$$NN_{MLP1}(\\mathbf{x})=g(\\mathbf{xW^1+b^1})\\mathbf{W^2}+\\mathbf{b^2}$$\n",
    "\n",
    "there $g$ is a non-linearity/activation function. We will come back to this later. For now, discuss with your neighbor: what are all the terms in the formula above, and how can you connect them to the picture above?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A feedforward neural network with 2 hidden layers:\n",
    "$$NN_{MLP2}(\\mathbf{x})=g^2(g^1(\\mathbf{xW^1+b^1})\\mathbf{W^2}+\\mathbf{b^2})\\mathbf{W^3}+\\mathbf{b^3}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The MLP2 is illustrated here (vertically). \n",
    "<div style=\"width: 35%;\">\n",
    "<img src=\"pics/nn_vertical.png\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div style=\"width: 15%;\">\n",
    "<img src=\"pics/nn_vertical.png\">\n",
    "</div>\n",
    "It is a bit cumbersome to see, so let's break the formula \n",
    "\n",
    "$$NN_{MLP2}(\\mathbf{x})=g^2(g^1(\\mathbf{xW^1+b^1})\\mathbf{W^2}+\\mathbf{b^2})\\mathbf{W^3}+\\mathbf{b^3}$$\n",
    "\n",
    "down into parts:\n",
    "\n",
    "$$\\mathbf{h^1}=g^1(\\mathbf{xW^1+b^1})$$\n",
    "$$\\mathbf{h^2}=g^2(\\mathbf{h_1W^2+b^2})$$\n",
    "$$NN_{MLP2}(\\mathbf{x})= \\mathbf{h^2}\\mathbf{W^3}+\\mathbf{b^3}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "## Notes on vector/matrix sizes:\n",
    "## x: 1x4 vector     W1: 4x6 matrix     b1: 1x6 vector \n",
    "## -> h1: 1x6   # first layer transforms 4 dimensional input into a 6 dimensional vector, etc\n",
    "## W2: 6x5 matrix    b2: 1x5 vector     -> h2: 1x5\n",
    "## W3: 5x3 matrix    b3: 1x3 vector     -> output: 1x3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Voilà! Now we have a description of a neural network, both graphically and algebraically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### References\n",
    "* More details in [Michael Nielsen's book chapter 1](http://neuralnetworksanddeeplearning.com/chap1.html)\n",
    "* Chapters 3 and 4 (pp. 37--49) of Y. Goldberg. 2007. Neural Network Methods for NLP. Or relevant sections in his primer: [A Primer on Neural Network Models for Natural Language Processing](http://arxiv.org/abs/1510.00726)\n",
    "* Sections 7.1 to 7.3 of D. Jurafsky and J. H. Martin. 2018. Speech and Language Processing."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
